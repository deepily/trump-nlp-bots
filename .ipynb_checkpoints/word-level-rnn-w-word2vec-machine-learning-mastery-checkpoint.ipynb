{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/ and https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/ and https://github.com/stanfordnlp/GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿SPEECH 1\n",
      "\n",
      "\n",
      "...Thank you so much.  That's so nice.  Isn't he a great guy.  He doesn't get a fair press; he doesn't get it.  It's just not fair.  And I have to tell you I'm here, and very strongly here\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc( filename ):\n",
    "    \n",
    "    # open the file as read only\n",
    "    file = open( filename, 'r' )\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# load document\n",
    "#in_filename = \"../texts/alice-in-wonderland.txt\"\n",
    "#in_filename = \"../texts/dr-zeuss-compilation.txt\"\n",
    "in_filename = \"../texts/trump-speeches.txt\"\n",
    "doc = load_doc( in_filename )\n",
    "print( doc[ :200 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc( doc, to_lower=True ):\n",
    "    \n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace( '--', ' ' )\n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    \n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans( '', '', string.punctuation )\n",
    "    tokens = [ w.translate( table ) for w in tokens ]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    if to_lower:\n",
    "        tokens = [ word for word in tokens if word.isalpha() ]\n",
    "    \n",
    "    # make lower case\n",
    "    tokens = [ word.lower() for word in tokens ] \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank', 'you', 'so', 'much', 'thats', 'so', 'nice', 'isnt', 'he', 'a', 'great', 'guy', 'he', 'doesnt', 'get', 'a', 'fair', 'press', 'he', 'doesnt', 'get', 'it', 'its', 'just', 'not', 'fair', 'and', 'i', 'have', 'to', 'tell', 'you', 'im', 'here', 'and', 'very', 'strongly', 'here', 'because', 'i', 'have', 'great', 'respect', 'for', 'steve', 'king', 'and', 'have', 'great', 'respect', 'likewise', 'for', 'citizens', 'united', 'david', 'and', 'everybody', 'and', 'tremendous', 'resect', 'for', 'the', 'tea', 'party', 'also', 'also', 'the', 'people', 'of', 'iowa', 'they', 'have', 'something', 'in', 'common', 'hardworking', 'people', 'they', 'want', 'to', 'work', 'they', 'want', 'to', 'make', 'the', 'country', 'great', 'i', 'love', 'the', 'people', 'of', 'iowa', 'so', 'thats', 'the', 'way', 'it', 'is', 'very', 'simple', 'with', 'that', 'said', 'our', 'country', 'is', 'really', 'headed', 'in', 'the', 'wrong', 'direction', 'with', 'a', 'president', 'who', 'is', 'doing', 'an', 'absolutely', 'terrible', 'job', 'the', 'world', 'is', 'collapsing', 'around', 'us', 'and', 'many', 'of', 'the', 'problems', 'weve', 'caused', 'our', 'president', 'is', 'either', 'grossly', 'incompetent', 'a', 'word', 'that', 'more', 'and', 'more', 'people', 'are', 'using', 'and', 'i', 'think', 'i', 'was', 'the', 'first', 'to', 'use', 'it', 'or', 'he', 'has', 'a', 'completely', 'different', 'agenda', 'than', 'you', 'want', 'to', 'know', 'about', 'which', 'could', 'be', 'possible', 'in', 'any', 'event', 'washington', 'is', 'broken', 'and', 'our', 'country', 'is', 'in', 'serious', 'trouble', 'and', 'total', 'disarray', 'very', 'simple', 'politicians', 'are', 'all']\n",
      "Total Tokens: 156162\n",
      "Unique Tokens: 5842\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_doc( doc )\n",
    "tokens_unique = list( set( tokens ) )\n",
    "print( tokens[ :200 ] )\n",
    "print( 'Total Tokens: %d' % len( tokens ) )\n",
    "print( 'Unique Tokens: %d' % len( tokens_unique ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 156111\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "sequence_len = 50 + 1\n",
    "sequences = list()\n",
    "\n",
    "for i in range( sequence_len, len( tokens ) ):\n",
    "    \n",
    "    # select sequence of tokens\n",
    "    seq = tokens[ i - sequence_len:i ]\n",
    "    \n",
    "    # convert into a line\n",
    "    line = ' '.join( seq )\n",
    "    \n",
    "    # store\n",
    "    sequences.append( line )\n",
    "    \n",
    "print( 'Total Sequences: %d' % len( sequences ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc( lines, filename ):\n",
    "    \n",
    "    data = '\\n'.join( lines )\n",
    "    file = open( filename, 'w' )\n",
    "    file.write( data )\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "#out_filename = \"../texts/dr-zeuss-compilation-sequences.txt\"\n",
    "out_filename = \"../texts/trump-speeches-sequences.txt\"\n",
    "save_doc( sequences, out_filename )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you so much thats so nice isnt he a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise',\n",
       " 'you so much thats so nice isnt he a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise for',\n",
       " 'so much thats so nice isnt he a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise for citizens',\n",
       " 'much thats so nice isnt he a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise for citizens united',\n",
       " 'thats so nice isnt he a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise for citizens united david',\n",
       " 'so nice isnt he a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise for citizens united david and',\n",
       " 'nice isnt he a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise for citizens united david and everybody',\n",
       " 'isnt he a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise for citizens united david and everybody and',\n",
       " 'he a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise for citizens united david and everybody and tremendous',\n",
       " 'a great guy he doesnt get a fair press he doesnt get it its just not fair and i have to tell you im here and very strongly here because i have great respect for steve king and have great respect likewise for citizens united david and everybody and tremendous resect']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in_filename = \"../texts/dr-zeuss-compilation-sequences.txt\"\n",
    "in_filename = \"../texts/trump-speeches-sequences.txt\"\n",
    "doc = load_doc( in_filename )\n",
    "lines = doc.split( '\\n' )\n",
    "lines[ 0:10 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Words to Index Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts( lines )\n",
    "sequences = tokenizer.texts_to_sequences( lines )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "156111\n"
     ]
    }
   ],
   "source": [
    "print( len( sequences[ 0 ] ) == sequence_len )\n",
    "print( len( sequences ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5842\n",
      "<class 'dict'>\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "print( len( tokenizer.word_index ) )\n",
    "print( type( tokenizer.word_index ) )\n",
    "print( tokenizer.word_index[ \"terrible\" ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5843"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len( tokenizer.word_index ) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate into input and output: for now it's 50 words input and 1 word output\n",
    "sequences = np.array( sequences )\n",
    "X = sequences[ :,:-1 ] # all rows, from word 0 up to, but not including, the last word\n",
    "y = sequences[ :,-1 ]  # all rows, last word only\n",
    "y = to_categorical( y, num_classes=vocab_size )\n",
    "seq_length = X.shape[ 1 ]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Filter GloVe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
      "Loaded 5644 word vectors.\n",
      "\n",
      "Words not found 198.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "embeddings_dimension = 300 #must be 50, 100, 200, 300\n",
    "glove = open( \"../glove/glove.6B.\" + str( embeddings_dimension ) + \"d.txt\" )\n",
    "\n",
    "for line in glove:\n",
    "    \n",
    "    values = line.split()\n",
    "    # 1st string is word...\n",
    "    word = values[ 0 ]\n",
    "    \n",
    "    if word in tokens_unique:\n",
    "        \n",
    "        # ...the rest are coefficients\n",
    "        coefs = np.asarray( values[ 1: ], dtype='float32' )\n",
    "        embeddings_index[ word ] = coefs\n",
    "        print( \"*\", end=\"\" )\n",
    "    \n",
    "glove.close()\n",
    "print( '\\nLoaded %s word vectors.' % len( embeddings_index ) )\n",
    "print( '\\nWords not found %d.' % ( len( tokenizer.word_index ) - len( embeddings_index ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform into Matrix Which Maps Coefs by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['selffunding',\n",
       " 'theyve',\n",
       " 'youve',\n",
       " 'beada',\n",
       " 'selfinspect',\n",
       " 'theyll',\n",
       " 'twoway',\n",
       " 'onethird',\n",
       " 'reince',\n",
       " 'lowenergy',\n",
       " 'bluecollar',\n",
       " 'ayeyeye',\n",
       " 'fbomb',\n",
       " 'obamaclinton',\n",
       " 'maralago',\n",
       " 'bigly',\n",
       " 'sixyear',\n",
       " 'escavators',\n",
       " 'nationbuilding',\n",
       " 'africanamerican',\n",
       " 'byebye',\n",
       " 'selfinspection',\n",
       " 'offmike',\n",
       " 'antiwoman',\n",
       " 'intelligencegathering',\n",
       " 'thatand',\n",
       " 'africanamericans',\n",
       " 'donaldjtrumpcom',\n",
       " 'mexicanamerican',\n",
       " 'hardhitting',\n",
       " 'braggadocious',\n",
       " 'selfpolice',\n",
       " 'komatsus',\n",
       " 'nobodys',\n",
       " 'taxexempt',\n",
       " 'indianas',\n",
       " 'fiveforone',\n",
       " 'oreilly',\n",
       " 'resect',\n",
       " 'everyones',\n",
       " 'theyd',\n",
       " 'cetain',\n",
       " 'romneycare',\n",
       " 'oneyard',\n",
       " 'threefooter',\n",
       " 'werent',\n",
       " 'clearsighted',\n",
       " 'nationstate',\n",
       " 'goodsized',\n",
       " 'airconditioner',\n",
       " 'disastertrump',\n",
       " 'nogood',\n",
       " 'peopletrump',\n",
       " 'exampletrump',\n",
       " 'brandnew',\n",
       " 'oldfashioned',\n",
       " 'wellover',\n",
       " 'trilliontrump',\n",
       " 'jobproducer',\n",
       " 'truthteller',\n",
       " 'expresident',\n",
       " 'middleincome',\n",
       " 'miniversion',\n",
       " 'lowlevel',\n",
       " 'smarttough',\n",
       " 'stronglooking',\n",
       " 'resonants',\n",
       " 'exofficials',\n",
       " 'incomptent',\n",
       " 'twentyfive',\n",
       " 'swenden',\n",
       " 'meand',\n",
       " 'friendand',\n",
       " 'hijackists',\n",
       " 'phenomenol',\n",
       " 'knifewielding',\n",
       " 'secondclass',\n",
       " 'orlandos',\n",
       " 'antiamerican',\n",
       " 'lawabiding',\n",
       " 'evergrowing',\n",
       " 'politicallycorrect',\n",
       " 'ityou',\n",
       " 'trumphe',\n",
       " 'leadingnevada',\n",
       " 'twoperson',\n",
       " 'nowand',\n",
       " 'securitybut',\n",
       " 'japanthey',\n",
       " 'showsi',\n",
       " 'gunhow',\n",
       " 'groupthey',\n",
       " 'peopleand',\n",
       " 'antiiowa',\n",
       " 'lovei',\n",
       " 'guyin',\n",
       " 'retweeted',\n",
       " 'realdonaldtrump',\n",
       " 'welleducated',\n",
       " 'underbudget',\n",
       " 'primarycaucus',\n",
       " 'informedthat',\n",
       " 'vetdogs',\n",
       " 'debatebecause',\n",
       " 'millionike',\n",
       " 'amongstand',\n",
       " 'startedi',\n",
       " 'oppositionwhat',\n",
       " 'pressi',\n",
       " 'gaveinvites',\n",
       " 'outsidego',\n",
       " 'waitlists',\n",
       " 'whyhe',\n",
       " 'landslidei',\n",
       " 'candidatewhen',\n",
       " 'thisif',\n",
       " 'halffrozen',\n",
       " 'themthis',\n",
       " 'debatesi',\n",
       " 'knowwe',\n",
       " 'betyou',\n",
       " 'landslidedoes',\n",
       " 'notesand',\n",
       " 'twoyear',\n",
       " 'nabsico',\n",
       " 'doddfrank',\n",
       " 'bewell',\n",
       " 'corruptness',\n",
       " 'countryyou',\n",
       " 'saidwhat',\n",
       " 'embargoing',\n",
       " 'sevenyearold',\n",
       " 'sevenyearsold',\n",
       " 'backtrillion',\n",
       " 'recordsetting',\n",
       " 'watchingwe',\n",
       " 'toothe',\n",
       " 'peoplebush',\n",
       " 'thereso',\n",
       " 'stateswhich',\n",
       " 'expresidents',\n",
       " 'runningthey',\n",
       " 'parttimers',\n",
       " 'nowthe',\n",
       " 'wellour',\n",
       " 'selffund',\n",
       " 'decisionsor',\n",
       " 'fantasticyou',\n",
       " 'semifairly',\n",
       " 'dday',\n",
       " 'permityou',\n",
       " 'scums',\n",
       " 'fairhaired',\n",
       " 'proisrael',\n",
       " 'lifechanging',\n",
       " 'mackeral',\n",
       " 'rollsroyce',\n",
       " 'schusters',\n",
       " 'oneminute',\n",
       " 'socialistcommunist',\n",
       " 'overbrimming',\n",
       " 'mortagage',\n",
       " 'deductability',\n",
       " 'eschelons',\n",
       " 'obselete',\n",
       " 'selffunded',\n",
       " 'ordierno',\n",
       " 'selffunder',\n",
       " 'bleh',\n",
       " 'ayeyayay',\n",
       " 'braggingly',\n",
       " 'sharons',\n",
       " 'fricking',\n",
       " 'itll',\n",
       " 'deathdefying',\n",
       " 'thirtythree',\n",
       " 'brexits',\n",
       " 'oversay',\n",
       " 'numberone',\n",
       " 'secondlowest',\n",
       " 'pences',\n",
       " 'dshtidnt',\n",
       " 'pliblg',\n",
       " 'maepgessenger',\n",
       " 'cameramans',\n",
       " 'nashltionally',\n",
       " 'kpzcompanies',\n",
       " 'thetory',\n",
       " 'thirdworld',\n",
       " 'alumisource',\n",
       " 'depressionlevel',\n",
       " 'politicianmade',\n",
       " 'upsidedown',\n",
       " 'taxfree',\n",
       " 'jobkilling',\n",
       " 'inflationadjusted',\n",
       " 'transpacificpartnership',\n",
       " 'americanproduced']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros( ( vocab_size, embeddings_dimension ) )\n",
    "missing_words = []\n",
    "\n",
    "# we need this to create empty coefficients array\n",
    "dummy_shape = embeddings_index[ \"the\" ].shape\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    \n",
    "    embedding_vector = embeddings_index.get( word )\n",
    "    \n",
    "    # not all words in our token list are in the wikipedia 400K set!\n",
    "    if embedding_vector is None:\n",
    "        \n",
    "        # report and create empty coefficients array\n",
    "        missing_words.append( word )\n",
    "        embedding_vector = np.zeros( dummy_shape )\n",
    "        \n",
    "    embedding_matrix[ i ] = embedding_vector\n",
    "    \n",
    "print( len( missing_words ) )\n",
    "missing_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm visually that \n",
    "print( len( embedding_matrix[ 0 ] ) )\n",
    "print( sum( embedding_matrix[ 0 ] ) )\n",
    "empty_coefficients_count = 0\n",
    "\n",
    "for i in range( len( embedding_matrix ) ):\n",
    "    if sum( embedding_matrix[ i ] ) == 0:\n",
    "        empty_coefficients_count += 1\n",
    "        \n",
    "empty_coefficients_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n",
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print( keras.__version__ )\n",
    "\n",
    "import tensorflow as tf\n",
    "print( tf.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           1752900   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           160400    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5843)              590143    \n",
      "=================================================================\n",
      "Total params: 2,593,943\n",
      "Trainable params: 2,593,943\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "# now using a pre-trained, non-trainable embedding from glove's wiki analysis\n",
    "model.add( Embedding( vocab_size, embeddings_dimension, weights=[embedding_matrix], input_length=seq_length, trainable=True ) )\n",
    "model.add( LSTM( seq_length * 2, return_sequences=True ) )\n",
    "model.add( LSTM( seq_length * 2 ) )\n",
    "model.add( Dense( seq_length * 2, activation='relu' ) )\n",
    "\n",
    "# fixed TypeError below, downgraded keras from 2.1.5 to 2.1.3: https://github.com/keras-team/keras/issues/9621\n",
    "# TypeError: softmax() got an unexpected keyword argument 'axis'\n",
    "model.add( Dense( vocab_size, activation='softmax' ) )\n",
    "\n",
    "print( model.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219.6171875\n",
      "151.85894941634243\n"
     ]
    }
   ],
   "source": [
    "# calc batch size\n",
    "print( len( sequences ) / 128 )\n",
    "print( len( sequences ) / 1028 )\n",
    "# Was:\n",
    "#batch_size = 128\n",
    "batch_size = 1028\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "156111/156111 [==============================] - 25s 159us/step - loss: 6.3837 - acc: 0.0334\n",
      "Epoch 2/300\n",
      "156111/156111 [==============================] - 23s 149us/step - loss: 6.0352 - acc: 0.0479\n",
      "Epoch 3/300\n",
      "156111/156111 [==============================] - 23s 150us/step - loss: 5.8541 - acc: 0.0608\n",
      "Epoch 4/300\n",
      "156111/156111 [==============================] - 23s 150us/step - loss: 5.6636 - acc: 0.0813\n",
      "Epoch 5/300\n",
      "156111/156111 [==============================] - 24s 151us/step - loss: 5.4961 - acc: 0.0998\n",
      "Epoch 6/300\n",
      "156111/156111 [==============================] - 24s 151us/step - loss: 5.3988 - acc: 0.1123\n",
      "Epoch 7/300\n",
      "156111/156111 [==============================] - 24s 151us/step - loss: 5.3025 - acc: 0.1226\n",
      "Epoch 8/300\n",
      "156111/156111 [==============================] - 24s 151us/step - loss: 5.1998 - acc: 0.1304\n",
      "Epoch 9/300\n",
      "156111/156111 [==============================] - 24s 151us/step - loss: 5.1089 - acc: 0.1399\n",
      "Epoch 10/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 5.0212 - acc: 0.1481\n",
      "Epoch 11/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.9368 - acc: 0.1549\n",
      "Epoch 12/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.8622 - acc: 0.1606\n",
      "Epoch 13/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.7927 - acc: 0.1652\n",
      "Epoch 14/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.7235 - acc: 0.1709\n",
      "Epoch 15/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.6540 - acc: 0.1767\n",
      "Epoch 16/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.5905 - acc: 0.1824\n",
      "Epoch 17/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.5298 - acc: 0.1862\n",
      "Epoch 18/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.4719 - acc: 0.1919\n",
      "Epoch 19/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.4163 - acc: 0.1960\n",
      "Epoch 20/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.3632 - acc: 0.2017\n",
      "Epoch 21/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.3097 - acc: 0.2051\n",
      "Epoch 22/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.2591 - acc: 0.2099\n",
      "Epoch 23/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.2071 - acc: 0.2144\n",
      "Epoch 24/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.1540 - acc: 0.2202\n",
      "Epoch 25/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.1056 - acc: 0.2247\n",
      "Epoch 26/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.0563 - acc: 0.2295\n",
      "Epoch 27/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 4.0080 - acc: 0.2350\n",
      "Epoch 28/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.9601 - acc: 0.2400\n",
      "Epoch 29/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.9121 - acc: 0.2450\n",
      "Epoch 30/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.8661 - acc: 0.2504\n",
      "Epoch 31/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.8191 - acc: 0.2553\n",
      "Epoch 32/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.7738 - acc: 0.2605\n",
      "Epoch 33/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.7294 - acc: 0.2652\n",
      "Epoch 34/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.6845 - acc: 0.2707\n",
      "Epoch 35/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.6397 - acc: 0.2766\n",
      "Epoch 36/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.5987 - acc: 0.2811\n",
      "Epoch 37/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.5557 - acc: 0.2864\n",
      "Epoch 38/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.5136 - acc: 0.2914\n",
      "Epoch 39/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.4748 - acc: 0.2968\n",
      "Epoch 40/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.4356 - acc: 0.3012\n",
      "Epoch 41/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.3965 - acc: 0.3080\n",
      "Epoch 42/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.3603 - acc: 0.3119\n",
      "Epoch 43/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.3227 - acc: 0.3178\n",
      "Epoch 44/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.2860 - acc: 0.3234\n",
      "Epoch 45/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.2491 - acc: 0.3278\n",
      "Epoch 46/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.2151 - acc: 0.3336\n",
      "Epoch 47/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.1823 - acc: 0.3387\n",
      "Epoch 48/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.1480 - acc: 0.3437\n",
      "Epoch 49/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.1159 - acc: 0.3485\n",
      "Epoch 50/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.0856 - acc: 0.3526\n",
      "Epoch 51/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.0519 - acc: 0.3583\n",
      "Epoch 52/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 3.0251 - acc: 0.3632\n",
      "Epoch 53/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.9903 - acc: 0.3673\n",
      "Epoch 54/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.9640 - acc: 0.3709\n",
      "Epoch 55/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.9334 - acc: 0.3768\n",
      "Epoch 56/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.9049 - acc: 0.3815\n",
      "Epoch 57/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.8768 - acc: 0.3862\n",
      "Epoch 58/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.8492 - acc: 0.3902\n",
      "Epoch 59/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.8228 - acc: 0.3952\n",
      "Epoch 60/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.7944 - acc: 0.3997\n",
      "Epoch 61/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.7710 - acc: 0.4031\n",
      "Epoch 62/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.7443 - acc: 0.4090\n",
      "Epoch 63/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.7207 - acc: 0.4120\n",
      "Epoch 64/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.6961 - acc: 0.4159\n",
      "Epoch 65/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.6706 - acc: 0.4209\n",
      "Epoch 66/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.6484 - acc: 0.4240\n",
      "Epoch 67/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.6270 - acc: 0.4283\n",
      "Epoch 68/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.6013 - acc: 0.4318\n",
      "Epoch 69/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.5783 - acc: 0.4367\n",
      "Epoch 70/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.5559 - acc: 0.4412\n",
      "Epoch 71/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.5359 - acc: 0.4430\n",
      "Epoch 72/300\n",
      "156111/156111 [==============================] - 24s 152us/step - loss: 2.5092 - acc: 0.4484\n",
      "Epoch 73/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.4909 - acc: 0.4521\n",
      "Epoch 74/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.4694 - acc: 0.4560\n",
      "Epoch 75/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.4476 - acc: 0.4602\n",
      "Epoch 76/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.4260 - acc: 0.4638\n",
      "Epoch 77/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.4038 - acc: 0.4683\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.3883 - acc: 0.4703\n",
      "Epoch 79/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.3694 - acc: 0.4744\n",
      "Epoch 80/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.3498 - acc: 0.4779\n",
      "Epoch 81/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.3289 - acc: 0.4813\n",
      "Epoch 82/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.3092 - acc: 0.4850\n",
      "Epoch 83/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.2921 - acc: 0.4885\n",
      "Epoch 84/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.2740 - acc: 0.4911\n",
      "Epoch 85/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.2545 - acc: 0.4954\n",
      "Epoch 86/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.2383 - acc: 0.4979\n",
      "Epoch 87/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.2224 - acc: 0.5010\n",
      "Epoch 88/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.2065 - acc: 0.5044\n",
      "Epoch 89/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.1896 - acc: 0.5073\n",
      "Epoch 90/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.1712 - acc: 0.5103\n",
      "Epoch 91/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.1504 - acc: 0.5156\n",
      "Epoch 92/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.1334 - acc: 0.5186\n",
      "Epoch 93/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.1158 - acc: 0.5219\n",
      "Epoch 94/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.1004 - acc: 0.5243\n",
      "Epoch 95/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.0831 - acc: 0.5278\n",
      "Epoch 96/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.0696 - acc: 0.5304\n",
      "Epoch 97/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.0547 - acc: 0.5338\n",
      "Epoch 98/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.0415 - acc: 0.5357\n",
      "Epoch 99/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.0273 - acc: 0.5383\n",
      "Epoch 100/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 2.0083 - acc: 0.5429\n",
      "Epoch 101/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.9935 - acc: 0.5451\n",
      "Epoch 102/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.9750 - acc: 0.5490\n",
      "Epoch 103/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.9611 - acc: 0.5532\n",
      "Epoch 104/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.9459 - acc: 0.5551\n",
      "Epoch 105/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.9334 - acc: 0.5582\n",
      "Epoch 106/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.9210 - acc: 0.5600\n",
      "Epoch 107/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.9034 - acc: 0.5643\n",
      "Epoch 108/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.8981 - acc: 0.5647\n",
      "Epoch 109/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.8825 - acc: 0.5684\n",
      "Epoch 110/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.8706 - acc: 0.5702\n",
      "Epoch 111/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.8584 - acc: 0.5722\n",
      "Epoch 112/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.8402 - acc: 0.5776\n",
      "Epoch 113/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.8289 - acc: 0.5795\n",
      "Epoch 114/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.8165 - acc: 0.5814\n",
      "Epoch 115/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.8034 - acc: 0.5842\n",
      "Epoch 116/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.7984 - acc: 0.5851\n",
      "Epoch 117/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.7913 - acc: 0.5860\n",
      "Epoch 118/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.7646 - acc: 0.5925\n",
      "Epoch 119/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.7533 - acc: 0.5953\n",
      "Epoch 120/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.7531 - acc: 0.5953\n",
      "Epoch 121/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.7344 - acc: 0.5985\n",
      "Epoch 122/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.7153 - acc: 0.6016\n",
      "Epoch 123/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.7071 - acc: 0.6042\n",
      "Epoch 124/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.7007 - acc: 0.6058\n",
      "Epoch 125/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.6803 - acc: 0.6085\n",
      "Epoch 126/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.6742 - acc: 0.6105\n",
      "Epoch 127/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.6619 - acc: 0.6141\n",
      "Epoch 128/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.6585 - acc: 0.6133\n",
      "Epoch 129/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.6434 - acc: 0.6163\n",
      "Epoch 130/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.6287 - acc: 0.6204\n",
      "Epoch 131/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.6197 - acc: 0.6229\n",
      "Epoch 132/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.6082 - acc: 0.6255\n",
      "Epoch 133/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5981 - acc: 0.6279\n",
      "Epoch 134/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5882 - acc: 0.6291\n",
      "Epoch 135/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5739 - acc: 0.6314\n",
      "Epoch 136/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5854 - acc: 0.6282\n",
      "Epoch 137/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5680 - acc: 0.6328\n",
      "Epoch 138/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5499 - acc: 0.6363\n",
      "Epoch 139/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5495 - acc: 0.6370\n",
      "Epoch 140/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5358 - acc: 0.6393\n",
      "Epoch 141/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5193 - acc: 0.6444\n",
      "Epoch 142/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5128 - acc: 0.6450\n",
      "Epoch 143/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.5035 - acc: 0.6472\n",
      "Epoch 144/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4846 - acc: 0.6514\n",
      "Epoch 145/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4738 - acc: 0.6538\n",
      "Epoch 146/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4701 - acc: 0.6542\n",
      "Epoch 147/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4715 - acc: 0.6538\n",
      "Epoch 148/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4640 - acc: 0.6548\n",
      "Epoch 149/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4507 - acc: 0.6581\n",
      "Epoch 150/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4387 - acc: 0.6610\n",
      "Epoch 151/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4301 - acc: 0.6623\n",
      "Epoch 152/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4233 - acc: 0.6649\n",
      "Epoch 153/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4242 - acc: 0.6633\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4291 - acc: 0.6603\n",
      "Epoch 155/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.4087 - acc: 0.6671\n",
      "Epoch 156/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3925 - acc: 0.6703\n",
      "Epoch 157/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3824 - acc: 0.6737\n",
      "Epoch 158/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3715 - acc: 0.6755\n",
      "Epoch 159/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3573 - acc: 0.6797\n",
      "Epoch 160/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3530 - acc: 0.6811\n",
      "Epoch 161/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3447 - acc: 0.6823\n",
      "Epoch 162/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3436 - acc: 0.6808\n",
      "Epoch 163/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3380 - acc: 0.6827\n",
      "Epoch 164/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3263 - acc: 0.6857\n",
      "Epoch 165/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3156 - acc: 0.6874\n",
      "Epoch 166/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3149 - acc: 0.6876\n",
      "Epoch 167/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3072 - acc: 0.6899\n",
      "Epoch 168/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.3077 - acc: 0.6891\n",
      "Epoch 169/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2963 - acc: 0.6915\n",
      "Epoch 170/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2849 - acc: 0.6948\n",
      "Epoch 171/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2814 - acc: 0.6943\n",
      "Epoch 172/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2875 - acc: 0.6930\n",
      "Epoch 173/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2793 - acc: 0.6944\n",
      "Epoch 174/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2815 - acc: 0.6943\n",
      "Epoch 175/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2601 - acc: 0.6993\n",
      "Epoch 176/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2473 - acc: 0.7018\n",
      "Epoch 177/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2292 - acc: 0.7070\n",
      "Epoch 178/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2199 - acc: 0.7095\n",
      "Epoch 179/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2203 - acc: 0.7092\n",
      "Epoch 180/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2175 - acc: 0.7096\n",
      "Epoch 181/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2058 - acc: 0.7120\n",
      "Epoch 182/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.2082 - acc: 0.7105\n",
      "Epoch 183/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1933 - acc: 0.7134\n",
      "Epoch 184/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1888 - acc: 0.7159\n",
      "Epoch 185/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1803 - acc: 0.7178\n",
      "Epoch 186/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1750 - acc: 0.7193\n",
      "Epoch 187/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1725 - acc: 0.7197\n",
      "Epoch 188/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1659 - acc: 0.7209\n",
      "Epoch 189/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1645 - acc: 0.7203\n",
      "Epoch 190/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1625 - acc: 0.7202\n",
      "Epoch 191/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1575 - acc: 0.7220\n",
      "Epoch 192/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1527 - acc: 0.7221\n",
      "Epoch 193/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1398 - acc: 0.7264\n",
      "Epoch 194/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1341 - acc: 0.7274\n",
      "Epoch 195/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1314 - acc: 0.7275\n",
      "Epoch 196/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1187 - acc: 0.7315\n",
      "Epoch 197/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1145 - acc: 0.7321\n",
      "Epoch 198/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1118 - acc: 0.7321\n",
      "Epoch 199/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1083 - acc: 0.7325\n",
      "Epoch 200/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.1007 - acc: 0.7346\n",
      "Epoch 201/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0954 - acc: 0.7352\n",
      "Epoch 202/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0891 - acc: 0.7371\n",
      "Epoch 203/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0928 - acc: 0.7364\n",
      "Epoch 204/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0868 - acc: 0.7371\n",
      "Epoch 205/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0832 - acc: 0.7385\n",
      "Epoch 206/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0704 - acc: 0.7419\n",
      "Epoch 207/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0825 - acc: 0.7374\n",
      "Epoch 208/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0634 - acc: 0.7420\n",
      "Epoch 209/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0460 - acc: 0.7469\n",
      "Epoch 210/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0355 - acc: 0.7507\n",
      "Epoch 211/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0381 - acc: 0.7490\n",
      "Epoch 212/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0673 - acc: 0.7398\n",
      "Epoch 213/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0655 - acc: 0.7398\n",
      "Epoch 214/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0460 - acc: 0.7445\n",
      "Epoch 215/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0307 - acc: 0.7504\n",
      "Epoch 216/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0084 - acc: 0.7562\n",
      "Epoch 217/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9982 - acc: 0.7598\n",
      "Epoch 218/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0005 - acc: 0.7569\n",
      "Epoch 219/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0152 - acc: 0.7535\n",
      "Epoch 220/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0007 - acc: 0.7570\n",
      "Epoch 221/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9882 - acc: 0.7602\n",
      "Epoch 222/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0082 - acc: 0.7537\n",
      "Epoch 223/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 1.0041 - acc: 0.7547\n",
      "Epoch 224/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9788 - acc: 0.7612\n",
      "Epoch 225/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9586 - acc: 0.7682\n",
      "Epoch 226/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9981 - acc: 0.7561\n",
      "Epoch 227/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9944 - acc: 0.7573\n",
      "Epoch 228/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9784 - acc: 0.7610\n",
      "Epoch 229/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9493 - acc: 0.7698\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9420 - acc: 0.7715\n",
      "Epoch 231/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9273 - acc: 0.7754\n",
      "Epoch 232/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9837 - acc: 0.7590\n",
      "Epoch 233/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9681 - acc: 0.7635\n",
      "Epoch 234/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9511 - acc: 0.7680\n",
      "Epoch 235/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9454 - acc: 0.7692\n",
      "Epoch 236/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9276 - acc: 0.7732\n",
      "Epoch 237/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9307 - acc: 0.7723\n",
      "Epoch 238/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9345 - acc: 0.7711\n",
      "Epoch 239/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9165 - acc: 0.7763\n",
      "Epoch 240/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9137 - acc: 0.7767\n",
      "Epoch 241/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8991 - acc: 0.7811\n",
      "Epoch 242/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9023 - acc: 0.7796\n",
      "Epoch 243/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8913 - acc: 0.7829\n",
      "Epoch 244/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8922 - acc: 0.7826\n",
      "Epoch 245/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8993 - acc: 0.7792\n",
      "Epoch 246/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9122 - acc: 0.7744\n",
      "Epoch 247/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9100 - acc: 0.7757\n",
      "Epoch 248/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9032 - acc: 0.7762\n",
      "Epoch 249/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9000 - acc: 0.7781\n",
      "Epoch 250/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9046 - acc: 0.7772\n",
      "Epoch 251/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8722 - acc: 0.7864\n",
      "Epoch 252/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8516 - acc: 0.7925\n",
      "Epoch 253/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8498 - acc: 0.7932\n",
      "Epoch 254/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8462 - acc: 0.7932\n",
      "Epoch 255/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8601 - acc: 0.7886\n",
      "Epoch 256/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8678 - acc: 0.7863\n",
      "Epoch 257/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8574 - acc: 0.7883\n",
      "Epoch 258/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8464 - acc: 0.7924\n",
      "Epoch 259/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.9110 - acc: 0.7724\n",
      "Epoch 260/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8844 - acc: 0.7802\n",
      "Epoch 261/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8528 - acc: 0.7895\n",
      "Epoch 262/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8232 - acc: 0.7990\n",
      "Epoch 263/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7984 - acc: 0.8074\n",
      "Epoch 264/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8083 - acc: 0.8029\n",
      "Epoch 265/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8399 - acc: 0.7923\n",
      "Epoch 266/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8289 - acc: 0.7955\n",
      "Epoch 267/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8385 - acc: 0.7925\n",
      "Epoch 268/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8214 - acc: 0.7976\n",
      "Epoch 269/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8089 - acc: 0.8021\n",
      "Epoch 270/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8224 - acc: 0.7968\n",
      "Epoch 271/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8198 - acc: 0.7977\n",
      "Epoch 272/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8090 - acc: 0.8003\n",
      "Epoch 273/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7915 - acc: 0.8057\n",
      "Epoch 274/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7783 - acc: 0.8099\n",
      "Epoch 275/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7728 - acc: 0.8113\n",
      "Epoch 276/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8010 - acc: 0.8021\n",
      "Epoch 277/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8021 - acc: 0.8009\n",
      "Epoch 278/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8020 - acc: 0.8008\n",
      "Epoch 279/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7881 - acc: 0.8050\n",
      "Epoch 280/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7775 - acc: 0.8082\n",
      "Epoch 281/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7719 - acc: 0.8092\n",
      "Epoch 282/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7560 - acc: 0.8146\n",
      "Epoch 283/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7694 - acc: 0.8090\n",
      "Epoch 284/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7842 - acc: 0.8062\n",
      "Epoch 285/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7866 - acc: 0.8044\n",
      "Epoch 286/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7879 - acc: 0.8036\n",
      "Epoch 287/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7541 - acc: 0.8145\n",
      "Epoch 288/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7502 - acc: 0.8155\n",
      "Epoch 289/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7851 - acc: 0.8043\n",
      "Epoch 290/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7829 - acc: 0.8037\n",
      "Epoch 291/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.8079 - acc: 0.7964\n",
      "Epoch 292/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7771 - acc: 0.8054\n",
      "Epoch 293/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7380 - acc: 0.8185\n",
      "Epoch 294/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7003 - acc: 0.8293\n",
      "Epoch 295/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.6991 - acc: 0.8303\n",
      "Epoch 296/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.6877 - acc: 0.8333\n",
      "Epoch 297/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.6967 - acc: 0.8296\n",
      "Epoch 298/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7100 - acc: 0.8245\n",
      "Epoch 299/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7195 - acc: 0.8217\n",
      "Epoch 300/300\n",
      "156111/156111 [==============================] - 24s 153us/step - loss: 0.7521 - acc: 0.8107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90d34510b8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile( loss='categorical_crossentropy', optimizer='adam', metrics=[ 'accuracy' ] )\n",
    "# fit model\n",
    "model.fit( X, y, batch_size=batch_size, epochs=500 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save( \"models/trump-speeches-02.keras\" )\n",
    "\n",
    "# save the tokenizer\n",
    "dump( tokenizer, open( \"tokenizers/trump-speeches-02.pkl\", 'wb' ) )\n",
    "\n",
    "# save embedding_matrix based on wiki embeddings, complete w/ missing coefficients array dummies\n",
    "dump( embedding_matrix, open( \"embeddings/trump-speeches-02.glove\", 'wb' ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use The Model to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = len( lines[ 0 ].split() ) - 1\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq( model, tokenizer, seq_length, seed_text, n_words ):\n",
    "    \n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    \n",
    "    # generate a fixed number of words\n",
    "    for _ in range( n_words ):\n",
    "        \n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences( [ in_text ] )[ 0 ] \n",
    "        \n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences( [ encoded ], maxlen=seq_length, truncating='pre' ) \n",
    "        \n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes( encoded, verbose=0 )\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break \n",
    "                \n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        \n",
    "        result.append( out_word )\n",
    "        \n",
    "    return ' '.join( result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart thing in the way they staged it well going to stage something properly also but as far as concerned i really care that much i just want a debate i think debating is a good thing healthy gets everything into the open but you want people like harwood that read...\n",
      "\n",
      "...a question blah blah blah that he wrote that he carefully wrote he thought he was such his career in my opinion his career is probably ruined or certainly threatened he could ask about anything you know he gave her his credit card you else nationwide with what he did\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[ randint( 0, len( lines ) ) ]\n",
    "print( seed_text + '...\\n' )\n",
    "#print( len( seed_text.split( \" \" ) ) )\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq( model, tokenizer, seq_length, seed_text, 50 )\n",
    "print( \"...\" + generated )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq_word_by_word( model, tokenizer, seq_length, seed_text, n_words ):\n",
    "    \n",
    "    print( \"...\", end='' )\n",
    "    #result = list()\n",
    "    in_text = seed_text\n",
    "    \n",
    "    # generate a fixed number of words\n",
    "    for _ in range( n_words ):\n",
    "        \n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences( [ in_text ] )[ 0 ] \n",
    "        \n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences( [ encoded ], maxlen=seq_length, truncating='pre' ) \n",
    "        \n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes( encoded, verbose=0 )\n",
    "        \n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                print( word, end=' ' )\n",
    "                break \n",
    "                \n",
    "        # append to input for next iteration\n",
    "        in_text += ' ' + out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the primary system running for president think of it thats more than ronald reagan who we love thats more than richard nixon its more than dwight d eisenhower you know he won the second world war in all fairness pretty good its more than the bushes but i mean by a...\n",
      "\n",
      "...lot and you turn with him and i watched my whole credit but they was given to any in beyond importing who on these vote "
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[ randint( 0, len( lines ) ) ]\n",
    "print( seed_text + '...\\n' )\n",
    "\n",
    "# generate new text\n",
    "generate_seq_word_by_word( model, tokenizer, seq_length, seed_text, 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how great is donald trump?\n",
      "...won a advertising person i say i had how but i was like the only celebrity i mean the heat probably makes me relax he "
     ]
    }
   ],
   "source": [
    "my_input = input()\n",
    "\n",
    "# generate new text\n",
    "generate_seq_word_by_word( model, tokenizer, seq_length, my_input, 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
